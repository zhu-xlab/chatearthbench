# ChatEarthBench: Benchmarking Multimodal Large Language Models for Earth Observation

**ChatEarthBench** is a benchmark for evaluating multimodal large language models (MLLMs) on Earth observation (EO) tasks under a **zero-shot setting**.

---

## Abstract

The recent advancements in multimodal large language models (MLLMs) offer new opportunities for Earth observation (EO) tasks by enhancing reasoning and analysis capabilities. However, fair and systematic evaluation of these models remains challenging. Existing assessments often suffer from dataset biases, which can lead to an overestimation of model performance and inconsistent comparisons across MLLMs.  

To address this issue, we introduce **ChatEarthBench**, a comprehensive benchmark dataset specifically designed for zero-shot evaluation of MLLMs in EO. ChatEarthBench comprises **10 image-text datasets spanning three data modalities**. Importantly, these datasets are **unseen by the evaluated MLLMs**, enabling rigorous and fair evaluation across diverse real-world EO tasks. Our analysis provides critical insights into the capabilities and limitations of current MLLMs, offering guidance for future EO-oriented model development.

---

## Website

ðŸ”— https://zhu-xlab.github.io/chatearthbench/

---


## Citation

```bibtex
@article{Yuan2026ChatEarthBench,
  title   = {ChatEarthBench: Benchmarking Multimodal Large Language Models for Earth Observation},
  author  = {Zhenghang Yuan and Zhitong Xiong and Thomas Dujardin and Xiang Li and Lichao Mou and Xiao Xiang Zhu},
  journal = {IEEE Geoscience and Remote Sensing Magazine},
  year    = {2026}
}
